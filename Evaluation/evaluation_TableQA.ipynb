{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-3hL1OGmDd0"
      },
      "source": [
        "# Visual-TableQA Evaluation Notebook\n",
        "----\n",
        "Evaluating Models on Synthetic Image Dataset on Tables for QA Reasoning and Recognition Tasks\n",
        "\n",
        "----\n",
        "authors: Marc Haraoui, Aser Lompo\n",
        "\n",
        "date: 07/06/2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3hElh_cuw92",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc3tJ-EqARg8",
        "outputId": "ca38daf7-e966-40be-80af-d354c230c208"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from typing import Dict, Any\n",
        "from groq import Groq\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "import google.generativeai as genai\n",
        "from evaluate import load\n",
        "from io import BytesIO\n",
        "import base64\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "import random\n",
        "from utils import extract_json_blocks, read_Exception\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcNXTPrgAoQd",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz-j6Lb72SlK"
      },
      "outputs": [],
      "source": [
        "# API keys for model hosts\n",
        "groq_key   = \"...\"\n",
        "google_key = \"...\"\n",
        "openai_key = \"...\"\n",
        "openrouter_key = \"...\"\n",
        "\n",
        "api_keys = {'groq': groq_key, 'google': google_key, 'openai': openai_key, 'openrouter': openrouter_key}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3AvHvbR4xEr",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Visual-TableQA Evaluation Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzmrpCGw3dKb"
      },
      "outputs": [],
      "source": [
        "class TableQA_Evaluation:\n",
        "    \"\"\"\n",
        "    Visual-TableQA Evaluation with VLMs-as-Juries System:\n",
        "    VLM jury system for evaluating model responses against ground truth answers.\n",
        "    Multiple VLMs act as juries giving binary scores (correct/incorrect) with majority voting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, api_keys: Dict[str, str]):\n",
        "        self.groq_client = Groq(api_key=api_keys['groq'])\n",
        "        genai.configure(api_key=api_keys['google'])\n",
        "        self.openai_client = OpenAI(api_key=api_keys['openai'])\n",
        "        self.openrouter_key = api_keys['openrouter']\n",
        "\n",
        "        self.models_spec = {\n",
        "            \"qwen/qwen3-32b\": {\"api\": \"groq\", \"temperature\": 0.6, \"top_p\": 0.95, \"json_format\": True},\n",
        "            \"deepseek-r1-distill-llama-70b\": {\"api\": \"groq\", \"temperature\": 0.6, \"top_p\": 0.95, \"json_format\": False},\n",
        "            \"gemini-2.5-pro\": {\"api\": \"google\", \"temperature\": 0.3, \"top_p\": 1.0, \"json_format\": True},\n",
        "            \"gpt-4.1\": {\"api\": \"openai\", \"temperature\": 0.1, \"top_p\": 0.9, \"json_format\": \"json_object\"},\n",
        "            \"deepseek/deepseek-prover-v2\": {\"api\": \"openrouter\", \"temperature\": 0.6, \"top_p\": 0.95, \"json_format\": True}\n",
        "        }\n",
        "\n",
        "        # Escape braces for Python format\n",
        "        self.prompt_template = (\"Evaluate if the answer matches the ground truth. \"\n",
        "                                \"To do so, read the question and determine whether the provided answer conveys the same meaning as the ground truth.\"\n",
        "                                \"Output a JSON response as follows:\\n\\n\"\n",
        "                                \"{{\\\"verdict\\\": 1}} for correct or {{\\\"verdict\\\": 0}} for incorrect.\\n\\n\"\n",
        "                                \"Question: {question}\\n\"\n",
        "                                \"Ground Truth: {ground_truth}\\n\"\n",
        "                                \"Answer: {prediction}\\n\\n\"\n",
        "                                \"Response:\")\n",
        "\n",
        "\n",
        "\n",
        "    def call_llm(self, model_name, prompt, max_tokens)-> Dict[str, Any]:\n",
        "        json_format = self.models_spec[model_name]['json_format']\n",
        "        api_host = self.models_spec[model_name]['api']\n",
        "        temperature, top_p = self.models_spec[model_name]['temperature'], self.models_spec[model_name]['top_p']\n",
        "\n",
        "        messages = prompt if api_host == 'google' else [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "        if api_host.lower() == \"groq\":\n",
        "            completion = self.groq_client.chat.completions.create(model=model_name,messages=messages,temperature=temperature, top_p= top_p,\n",
        "              max_completion_tokens=max_tokens,stream=False, reasoning_format='hidden' if model_name in [\"deepseek-r1-distill-llama-70b\"] else None,\n",
        "              response_format={\"type\": \"json_object\"} if json_format else None,\n",
        "            )\n",
        "            response = completion.choices[0].message.content\n",
        "            if completion.choices[0].finish_reason == \"stop\":\n",
        "                return response if json_format else extract_json_blocks(response)\n",
        "            else:\n",
        "                print(completion.choices[0].finish_reason)\n",
        "                raise Exception(f\"API call ended before task. Reason: {completion.choices[0].finish_reason}\")\n",
        "\n",
        "        elif api_host.lower() == \"google\":\n",
        "            model = genai.GenerativeModel(model_name)\n",
        "            generation_config = {\n",
        "                                \"max_output_tokens\": max_tokens,\n",
        "                                \"temperature\": temperature,\n",
        "                                \"top_p\": top_p,\n",
        "                                \"response_mime_type\": \"application/json\" if json_format else None,\n",
        "                                }\n",
        "            response = model.generate_content(contents=messages, generation_config=generation_config)\n",
        "            if (response.prompt_feedback is None or\n",
        "                  response.prompt_feedback.block_reason.name == \"BLOCK_REASON_UNSPECIFIED\"):\n",
        "                return response.text if json_format else extract_json_blocks(response.text)\n",
        "            else:\n",
        "                print(response.prompt_feedback.block_reason.name)\n",
        "                raise Exception(f\"API call ended before task. Reason: {response.prompt_feedback.block_reason.name}\")\n",
        "\n",
        "        elif api_host.lower() == \"openai\":\n",
        "            completion = self.openai_client.chat.completions.create(model=model_name,messages=messages,temperature=temperature,\n",
        "                    top_p= top_p, max_completion_tokens=max_tokens,response_format= {\"type\": json_format})\n",
        "\n",
        "            if completion.choices[0].finish_reason == \"stop\":\n",
        "                return completion.choices[0].message.content\n",
        "            else:\n",
        "                print(completion.choices[0].finish_reason)\n",
        "                raise Exception(f\"API call ended before task. Reason: {completion.choices[0].finish_reason}\")\n",
        "\n",
        "        elif api_host.lower() == \"openrouter\":\n",
        "            if model_name == 'thudm/glm-z1-32b:free':\n",
        "                args = {\"model\": model_name, \"messages\": messages}\n",
        "            else:\n",
        "                args = {\"model\": model_name, \"messages\": messages, \"temperature\": temperature, \"top_p\": top_p, \"response_format\": {\"type\": \"json_object\"}}\n",
        "\n",
        "            response = requests.post(url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "                                 headers={\"Authorization\": f\"Bearer {self.openrouter_key}\", \"Content-Type\": \"application/json\"},\n",
        "                                   data=json.dumps(args))\n",
        "            response = response.json()\n",
        "            #print(response)\n",
        "            if response['choices'][0]['finish_reason'] == \"stop\":\n",
        "                return extract_json_blocks(response['choices'][0]['message']['content'])\n",
        "            else:\n",
        "                print(response['choices'][0]['finish_reason'])\n",
        "                raise Exception(f\"API call ended before task. Reason: {response['choices'][0]['finish_reason']}\")\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"Please choose api cloud host from 'google', 'groq', 'openai' and 'openrouter'.\")\n",
        "\n",
        "    def safe_call_llm(self, model_name, prompt, max_tokens=5000):\n",
        "      time.sleep(random.uniform(1, 3))\n",
        "      try:\n",
        "          return self.call_llm(model_name, prompt, max_tokens), None\n",
        "      except Exception as e:\n",
        "          # Handle API calls limits\n",
        "          msg = read_Exception(e).lower()\n",
        "          if any(keyword in msg for keyword in [\"quota\", \"billing\", \"insufficient\", \"resource exhausted\", \"429\", \"402\"]):\n",
        "              print(model_name, \"has reached its limit\")\n",
        "          elif 'ended' in msg:\n",
        "              print(model_name, \"needs more tokens to think\")\n",
        "          return None, msg\n",
        "\n",
        "\n",
        "    def evaluate_single_jury(self, model_name: str, question: str, prediction: str, ground_truth: str) -> Dict[str, Any]:\n",
        "        prompt = self.prompt_template.format(question=question, prediction=prediction, ground_truth=ground_truth)\n",
        "        for attempt in range(3):\n",
        "            decision, call_status = self.safe_call_llm(model_name=model_name, prompt=prompt)\n",
        "            if call_status is None:\n",
        "                decision = json.loads(decision)\n",
        "                verdict = decision['verdict']\n",
        "                return {\"jury_model\": model_name, \"verdict\": verdict, \"success\": True}\n",
        "        # Exceeded retries, skip this jury\n",
        "        return {\"jury_model\": model_name, \"verdict\": 0, \"success\": False, \"error\": call_status}\n",
        "\n",
        "    def compute_llm_juries(self, question: str, pred: str, ref: str) -> Dict[str, Any]:\n",
        "        evaluations = {model: self.evaluate_single_jury(model, question, pred, ref)\n",
        "                       for model in self.models_spec}\n",
        "\n",
        "        # Keep only successful juries\n",
        "        successful = [eval for _,eval in evaluations.items() if eval.get(\"success\")]\n",
        "        verdicts = [eval[\"verdict\"] for eval in successful]\n",
        "        if not verdicts:\n",
        "            majority = None\n",
        "            print(\"no verdict\")\n",
        "        elif verdicts.count(1) > verdicts.count(0):\n",
        "            majority = 1\n",
        "        elif verdicts.count(1) < verdicts.count(0):\n",
        "            majority = 0\n",
        "        else:\n",
        "            majority = evaluations[\"gemini-2.5-pro\"][\"verdict\"]\n",
        "\n",
        "        confidence = 0.0 if not verdicts else verdicts.count(majority) / len(verdicts)\n",
        "\n",
        "        return {\n",
        "            \"evaluations\": evaluations,\n",
        "            \"final_verdict\": majority,\n",
        "            \"confidence\": confidence,\n",
        "            \"successful_juries\": len(successful),\n",
        "            \"total_juries\": len(evaluations)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "yMaV-FKCeHcD"
      },
      "source": [
        "## Functions used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzt-Z_qzeHcD"
      },
      "outputs": [],
      "source": [
        "def convert_image_to_bytes(image, format=\"JPEG\"):\n",
        "    if image.mode == \"RGBA\":\n",
        "        image = image.convert(\"RGB\")  # remove alpha channel for JPEG\n",
        "    buffer = BytesIO()\n",
        "    image.save(buffer, format=format)\n",
        "    return buffer, buffer.getvalue()\n",
        "\n",
        "\n",
        "def encode_image_to_b64(image):\n",
        "    buffer, _ = convert_image_to_bytes(image)\n",
        "    return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
        "\n",
        "\n",
        "# LLama 4 Models Function\n",
        "def llama4_vision(query, image, model_name, client):\n",
        "    b64_image = encode_image_to_b64(image)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": query},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64_image}\"},\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model_name, messages=messages,\n",
        "        temperature=0.6, max_completion_tokens=2000,\n",
        "        top_p=0.95, stream=False,\n",
        "    )\n",
        "\n",
        "    response = completion.choices[0].message.content\n",
        "    if completion.choices[0].finish_reason == \"stop\":\n",
        "        return response\n",
        "    else:\n",
        "        print(completion.choices[0].finish_reason)\n",
        "        raise Exception(f\"API call ended before task. Reason: {completion.choices[0].finish_reason}\")\n",
        "\n",
        "\n",
        "# Gemini Model Function\n",
        "def gemini_vision(query, image, model_name, client):\n",
        "    _, image_encoded = convert_image_to_bytes(image)\n",
        "    generation_config = {\"max_output_tokens\": 2000, \"temperature\": 0.3}\n",
        "    messages = [\n",
        "        {\"mime_type\": \"image/jpeg\", \"data\": image_encoded},\n",
        "        query\n",
        "    ]\n",
        "    response = client.generate_content(\n",
        "        contents=messages,\n",
        "        generation_config=generation_config\n",
        "    )\n",
        "    if (response.prompt_feedback is None or\n",
        "                response.prompt_feedback.block_reason.name == \"BLOCK_REASON_UNSPECIFIED\"):\n",
        "        return response.text\n",
        "    else:\n",
        "        print(response.prompt_feedback.block_reason.name)\n",
        "        raise Exception(f\"API call ended before task. Reason: {response.prompt_feedback.block_reason.name}\")\n",
        "\n",
        "\n",
        "# OpenAI Model Function\n",
        "def openai_vision(query, image, model_name, client):\n",
        "    b64_image = encode_image_to_b64(image)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": query},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64_image}\"},\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages,\n",
        "        temperature=0.2,\n",
        "        max_completion_tokens=2000,\n",
        "    )\n",
        "    if completion.choices[0].finish_reason == \"stop\":\n",
        "        return completion.choices[0].message.content\n",
        "    else:\n",
        "        print(completion.choices[0].finish_reason)\n",
        "        raise Exception(f\"API call ended before task. Reason: {completion.choices[0].finish_reason}\")\n",
        "\n",
        "# Open Router Model Function\n",
        "def openrouter_vision(query, image, model_name, client):\n",
        "    b64_image = encode_image_to_b64(image)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": query},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64_image}\"},\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages,\n",
        "        temperature=0.4,\n",
        "        max_completion_tokens=2000,\n",
        "    )\n",
        "    if completion.choices[0].finish_reason == \"stop\":\n",
        "        return completion.choices[0].message.content\n",
        "    else:\n",
        "        print(completion.choices[0].finish_reason)\n",
        "        raise Exception(f\"API call ended before task. Reason: {completion.choices[0].finish_reason}\")\n",
        "\n",
        "\n",
        "def generate_content(query, image, model_name, client, api):\n",
        "    try:\n",
        "        if api==\"groq\":\n",
        "            return llama4_vision(query, image, model_name, client), None\n",
        "        elif api==\"google\":\n",
        "            return gemini_vision(query, image, model_name, client), None\n",
        "        elif api==\"openai\":\n",
        "            return openai_vision(query, image, model_name, client), None\n",
        "        elif api==\"openrouter\":\n",
        "                return openrouter_vision(query, image, model_name, client), None\n",
        "        else:\n",
        "            raise Exception(\"API client not found\")\n",
        "    except Exception as e:\n",
        "        # Handle API calls limits\n",
        "        msg = read_Exception(e).lower()\n",
        "        if any(keyword in msg for keyword in [\"quota\", \"billing\", \"insufficient\", \"resource exhausted\", \"429\", \"402\"]):\n",
        "            print(model_name, \"has reached its limit\")\n",
        "        elif 'ended' in msg:\n",
        "            print(model_name, \"needs more tokens to think\")\n",
        "        else:\n",
        "            print(msg)\n",
        "        return None, msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFcGDlJazlJ1",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Model Evaluation using a dataset.\n",
        "\n",
        "you can replace Visual-TableQA dataset by any dataset you need"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLuLlnXd-cQH"
      },
      "source": [
        "#### Setup up available models and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q88FtG-LeHcK",
        "outputId": "9b910b2d-eda8-4276-94d4-555e055b151c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 100%|██████████| 3604/3604 [00:03<00:00, 1036.80 examples/s]\n",
            "Generating validation split: 100%|██████████| 777/777 [00:00<00:00, 1277.15 examples/s]\n",
            "Generating test split: 100%|██████████| 784/784 [00:00<00:00, 1225.22 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['image', 'question', 'answer'],\n",
            "    num_rows: 784\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset('AI-4-Everyone/Visual-TableQA', split='test')\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n5rhqF14dJ3"
      },
      "outputs": [],
      "source": [
        "models_config = {\n",
        "    # Groq\n",
        "    \"meta-llama/llama-4-scout-17b-16e-instruct\": \"groq\",\n",
        "    \"meta-llama/llama-4-maverick-17b-128e-instruct\": \"groq\",\n",
        "\n",
        "    # Google\n",
        "    \"gemini-2.5-flash\":\"google\",\n",
        "    \"gemini-2.5-pro\": \"google\",\n",
        "\n",
        "    # OpenAI\n",
        "    \"openai/gpt-4o\": \"openrouter\",\n",
        "    \"openai/gpt-4o-mini\": \"openrouter\",\n",
        "    \"anthropic/claude-3.5-sonnet\": \"openrouter\",\n",
        "\n",
        "    # Open Router\n",
        "    \"opengvlab/internvl3-14b:free\": \"openrouter\",\n",
        "    \"qwen/qwen2.5-vl-32b-instruct:free\": \"openrouter\",\n",
        "    \"mistralai/mistral-small-3.1-24b-instruct:free\":\"openrouter\",\n",
        "    \"google/gemma-3-27b-it:free\": \"openrouter\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke-_xS0l97VS"
      },
      "source": [
        "#### Function to select provider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gsWOkNt9-fM"
      },
      "outputs": [],
      "source": [
        "def api_provider(model_name):\n",
        "    api = models_config[model_name]\n",
        "    if api == \"groq\":\n",
        "        client = Groq(api_key=api_keys['groq'])\n",
        "    elif api == \"google\":\n",
        "        genai.configure(api_key=api_keys['google'])\n",
        "        client = genai.GenerativeModel(model_name)\n",
        "    elif api == \"openai\":\n",
        "        client = OpenAI(api_key=api_keys['openai'])\n",
        "    elif api == \"openrouter\":\n",
        "        client = OpenAI(\n",
        "                    api_key=api_keys['openrouter'],\n",
        "                    base_url=\"https://openrouter.ai/api/v1\"\n",
        "              )\n",
        "    else:\n",
        "      raise Exception(rf\"{model_name} is unknown.\")\n",
        "    return client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoU3DeEp44-q"
      },
      "outputs": [],
      "source": [
        "# Instantiate the evaluator\n",
        "evaluator = TableQA_Evaluation(api_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox8V7WmBC6K9"
      },
      "outputs": [],
      "source": [
        "#model_name = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
        "#model_name = \"meta-llama/llama-4-maverick-17b-128e-instruct\"\n",
        "#model_name = \"gemini-2.5-flash\"\n",
        "#model_name = \"gemini-2.5-pro\"\n",
        "model_name = \"openai/gpt-4o\"\n",
        "#model_name = \"openai/gpt-4o-mini\"\n",
        "# model_name = \"opengvlab/internvl3-14b:free\"\n",
        "#model_name = \"qwen/qwen2.5-vl-32b-instruct:free\"\n",
        "#model_name = \"mistralai/mistral-small-3.1-24b-instruct:free\"\n",
        "#model_name = \"google/gemma-3-27b-it:free\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j6iGUJgeHcM"
      },
      "outputs": [],
      "source": [
        "# Metrics record\n",
        "\"\"\"\n",
        "results = {\n",
        "    \"correctness\": [],\n",
        "    \"skipped\":[],\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "with open(\"gpt-4o-results.json\", \"r\") as f:\n",
        "    results = json.load(f)\n",
        "#\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMh7cxeAzofC",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Client\n",
        "client = api_provider(model_name)\n",
        "api = models_config[model_name]\n",
        "\n",
        "# Batch size and number of batches\n",
        "b_size = 4\n",
        "num_b  = len(dataset)//b_size\n",
        "\n",
        "# Start evaluation\n",
        "for batch in range(num_b):\n",
        "    skip_batch = False\n",
        "    batch_correctness = []\n",
        "    for i in range(batch*b_size, (batch+1)*b_size):\n",
        "        # Retrieve data from row\n",
        "        image = dataset[i]['image']\n",
        "        question = dataset[i]['question']\n",
        "        answer = dataset[i]['answer']\n",
        "\n",
        "        # Predict response\n",
        "        pred, verdict = None, None\n",
        "        for attempt in range(3):\n",
        "            try:\n",
        "                time.sleep(random.uniform(1, 3)) # Sleep for a few seconds to not exceed max requests per min\n",
        "                pred, _ = generate_content(question, image, model_name, client, api)\n",
        "                break # success\n",
        "            except Exception as e:\n",
        "                if attempt == 2:\n",
        "                    print(f\"Issue in batch {batch}, row {i}: {e}\")\n",
        "        if pred is None:\n",
        "            # at least one attempt failed → skip the whole batch\n",
        "            print(f\"Skipping entire batch {batch} due to failure at row {i}.\")\n",
        "            skip_batch = True\n",
        "            break  # break the iteration loop\n",
        "\n",
        "        try:\n",
        "            verdict = evaluator.compute_llm_juries(question, pred, answer)['final_verdict']\n",
        "        except Exception as e:\n",
        "            print(e.__class__.__name__, \" An Error occured while computing juries verdict\")\n",
        "        if verdict is None:\n",
        "            # at least one jury failed → skip the whole batch\n",
        "            print(f\"Skipping entire batch {batch} due to failure at row {i}.\")\n",
        "            skip_batch = True\n",
        "            break  # break the iteration loop\n",
        "\n",
        "        batch_correctness.append(verdict)\n",
        "\n",
        "    if skip_batch:\n",
        "        results['skipped'].append(batch)\n",
        "        continue  # Skip evaluation for this batch and move to the next one\n",
        "\n",
        "    # Evaluate model with metrics\n",
        "    batch_correctness = sum(batch_correctness) / b_size\n",
        "\n",
        "    results['correctness'].append(batch_correctness)\n",
        "\n",
        "    print(f\"------Batch {batch} ----------\")\n",
        "    print(f\"Correctness {results['correctness'][-1]}\")\n",
        "    with open(\"gpt-4o-results.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6VPv_4GOpkQ"
      },
      "source": [
        "#### Final results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeB5muGFOVDN"
      },
      "outputs": [],
      "source": [
        "# Final results\n",
        "\n",
        "for record in [\"gemini-2.5-flash-results.json\", \"gemini-2.5-pro-results.json\", \"gpt-4o-results.json\",\n",
        "                    \"gpt-4o-mini-results.json\", \"llama-4-16e-results.json\", \"llama-4-128e-results.json\",\n",
        "                    \"mistral-small-3.1-24b-results.json\", \"qwen2.5-vl-32b-results.json\"]:\n",
        "    with open(record, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "\n",
        "    num_b = len(results[\"correctness\"])\n",
        "    results[\"correctness\"] = sum(results[\"correctness\"]) /num_b\n",
        "\n",
        "    # Print results\n",
        "    print(f'{record[:-5]} \\n \\\n",
        "     * correctness: {results[\"correctness\"]*100:.2f}%.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gkv6uGpeHcO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "m3hElh_cuw92",
        "mcNXTPrgAoQd",
        "q3AvHvbR4xEr"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
