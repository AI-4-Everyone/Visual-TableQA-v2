{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-3hL1OGmDd0"
   },
   "source": [
    "# Synthetic Dataset Generator: TableQA\n",
    "----\n",
    "Synthetic Image Dataset on Tables for QA Reasoning and Recognition Tasks\n",
    "\n",
    "----\n",
    "authors: Marc Haraoui, Aser Lompo\n",
    "\n",
    "date: 19/03/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AdJboK7u4Y1"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
      "Collecting groq\n",
      "  Downloading groq-0.31.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq) (4.13.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
      "Downloading groq-0.31.1-py3-none-any.whl (134 kB)\n",
      "Installing collected packages: groq\n",
      "  Attempting uninstall: groq\n",
      "    Found existing installation: groq 0.24.0\n",
      "    Uninstalling groq-0.24.0:\n",
      "      Successfully uninstalled groq-0.24.0\n",
      "Successfully installed groq-0.31.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3hElh_cuw92"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ahRpc7ANuyG-",
    "outputId": "97cfabc1-fbd3-4af1-e86e-3fbc6fc84085",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from PIL import Image as PILImage\n",
    "from collections import Counter\n",
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "from IPython.display import Image, display\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "from typing_extensions import final\n",
    "import random\n",
    "from pprint import pprint\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G6NVmxg29Sl7"
   },
   "outputs": [],
   "source": [
    "from utils import generate_unique_filename, crop_image, save_latex_table_as_image, extract_json_blocks, decode_llm_latex_output, decode_control_sequences, extract_tables_from_text, update_start_time, read_Exception\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkzrC06Lyi1V"
   },
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wA1_EdQl5KpC"
   },
   "outputs": [],
   "source": [
    "# API keys for model hosts\n",
    "groq_key   = \"...\"\n",
    "google_key = \"...\"\n",
    "openai_key = \"...\"\n",
    "openrouter_key = \"...\"\n",
    "\n",
    "\n",
    "# LATEX INSTRUCTIONS\n",
    "TABLE_INSTRUCT_LATEX = r\"\"\"\n",
    "You are an expert in generating synthetic datasets composed of LaTeX-formatted tables, optionally accompanied by illustrative diagrams. Your task is to produce structured content suitable for data-centric documents, ensuring each table (and diagram, if included) is clear, well-organized, and visually informative.\n",
    "Your final output should start with ```json and end with ``` as plain text, not just formatting. Like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"table_1\": \"BEGIN_LATEX\n",
    "<LaTeX code for table 1 (with/without diagram) here>\n",
    "END_LATEX\",\n",
    "\n",
    "  \"table_2\": \"BEGIN_LATEX\n",
    "<LaTeX code for table 2 (with/without diagram) here>\n",
    "END_LATEX\",\n",
    "\n",
    "  \"table_3\": \"BEGIN_LATEX\n",
    "<LaTeX code for table 3 (with/without diagram) here>\n",
    "END_LATEX\"\n",
    "}\n",
    "```\n",
    "\n",
    "Requirements:\n",
    "    The tables and diagrams will be used to generate reasoning questions. Therefore:\n",
    "\n",
    "        - If topic inspirations are supplied, ensure every generated table aligns with those topics.\n",
    "\n",
    "        - Each LaTeX output must primarily consist of a table. Include a diagram only if it meaningfully complements the table; avoid adding one unnecessarily. Do not generate diagrams alone. If a diagram is empty or non necessary DON'T INCLUDE it.\n",
    "\n",
    "        - Keep any diagram minimal—smaller than the table, chart-free, and purely illustrative—serving only to reinforce the table’s content without adding new information.\n",
    "\n",
    "        - Each table and their diagram must contain realistic, domain-relevant content. They must be self-contained, include a clear descriptive title and not rely on external data to compile.\n",
    "\n",
    "        - The type of information presented should be diverse—such as numerical data or qualitative. The variety and richness of visual elements is essential to the overall quality of the table and their diagram. Table quality should also come with a large number of rows and columns.\n",
    "\n",
    "        - Table and diagram layouts should be creatively designed—taking inspiration from reference example (when provided) but incorporating meaningful variations such as colors, multi-row or multi-column cells, custom formatting adjustments, or any other visual enhancement that promotes structural diversity.\n",
    "\n",
    "        - Table layouts should be at least as complex as the example provided, don't try to simplify (diagrams are not mandatory). Table complexity should also come with a large number of rows and columns.\n",
    "\n",
    "        - Do NOT escape any characters in the LaTeX code. The LaTeX must be written as plain text, exactly as it would appear in a .tex file, with real line breaks and single backslashes (\\), not JSON-escaped.\n",
    "\n",
    "        - All LaTeX tables and diagrams must be constrained to fit entirely within the printable area of a standard A4 page when compiled to PDF, without overflowing horizontally or vertically. Use appropriate formatting techniques such as adjusting column widths, reducing font size, or enabling landscape mode if necessary but NEVER rotation.\n",
    "\n",
    "        - Make sure each LaTeX table and diagram includes all required \\usepackage declarations and is enclosed within a complete, compilable LaTeX document structure, including the appropriate preamble and \\begin{document}...\\end{document} block.\n",
    "\n",
    "        - Make sure each LaTeX codes start and end with BEGIN_LATEX and END_LATEX, respectively.\n",
    "\n",
    "        - Make sure to wrapp your final answer with ```json at the beginning and ``` at the end.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jbdIKJqlGiq7"
   },
   "outputs": [],
   "source": [
    "QA_INSTRUCT = r\"\"\"\n",
    "You are an expert in generating question–answer pairs from LaTeX-formatted. Your task is to create a structured dataset consisting of visually challenging, reasoning-based questions and their corresponding answers derived from a given LaTeX formatted table with optional diagram.\n",
    "\n",
    "Input:\n",
    "\n",
    "You will be provided with a sample LaTeX table as context. Based on this table or diagram, your goal is to generate a JSON object with the following structure:\n",
    "\n",
    "    questions: A python list of 3 challenging questions that require reasoning and analysis based ONLY on the data presented in the table and the optional diagram. The questions must be answerable using ONLY the information in the table or diagram(no extra knowledge).\n",
    "    answers: A python list of 3 detailed answers to the 3 questions, including a clear chain of thought explaining the reasoning process.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "    All questions must be relevant to the table's context and designed to test deeper understanding or inference.\n",
    "    When possible, all questions should make full use of the visual or structural elements of the table or diagram (such as rows, columns, headers, colors, patterns, diagrams etc.) while maintaining clear relevance to the table’s content.\n",
    "    Questions must be clear and answarable with an objective methodology, no subjective question.\n",
    "    All entries (both questions and answers) should be returned as lists of string values.\n",
    "    The global result should be a single JSON object wrapped in a markdown code block using ```json at the beginning and ``` at the end, and containing all two key-value pairs.\n",
    "    This means your output should start with ```json and end with ``` as plain text, not just formatting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "okG-L2x8QQIV"
   },
   "outputs": [],
   "source": [
    "QA_EVAL_INSTRUCT = r\"\"\"\n",
    "You are a reasoning question answer expert. You will be given a LaTeX formated table with/without diagram, a list of 3 topics, and a pair of a question and its answer.\n",
    "\n",
    "Your task is to evaluate the pair of question answer based solely on the data in the LaTeX code and these criteria:\n",
    "\n",
    "    1) Does the LaTeX code contain a Table (not some charts alone or diagrams alone) ?\n",
    "\n",
    "    2) Does the table, any optional diagrams, and the rest of the document are on one single topic from the provided list of topics, and internally consistent (be careful to off-topic diagrams)?\n",
    "\n",
    "    3) Is the question clear and related to the table or the diagram?\n",
    "\n",
    "    4) Is the answer (including its reasoning) totally valid and does it actually respond to the question?\n",
    "\n",
    "    5) Is the answer FULLY supported by and ONLY BY the table or diagram data (no extra knowledge)?\n",
    "\n",
    "If the five criteria are true, mark the pair as correct.\n",
    "If one of the criteria is not met, mark it as incorrect.\n",
    "\n",
    "Think step by step and conclude with your decision and the index of the criterium not met (if none, index is 0) as follows:\n",
    "JSON_mention\n",
    "{{\"decision\": [0, index_of_the_criterium_not_met]}} for incorrect or {{\"decision\": [1, 0]}} for correct\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "2pSbt7UQ9Sl-"
   },
   "outputs": [],
   "source": [
    "generation_settings = {\n",
    "    \"qwen/qwen3-32b\": {\n",
    "        \"temperature\": 0.2, \"top_p\": 0.95\n",
    "    },\n",
    "    \"deepseek-r1-distill-llama-70b\": {\n",
    "        \"temperature\": 0.2, \"top_p\": 0.95\n",
    "    },\n",
    "    \"gemini-2.5-flash-preview-04-17\": {\n",
    "        \"temperature\": 0.3, \"top_p\": 1.0\n",
    "    },\n",
    "    \"gpt-4.1-mini\": {\n",
    "        \"temperature\": 0.3, \"top_p\": 1.0\n",
    "    },\n",
    "    \"deepseek/deepseek-prover-v2:free\": {\n",
    "        \"temperature\": 0.2, \"top_p\": 0.95\n",
    "    },\n",
    "    \"deepseek/deepseek-prover-v2\": {\n",
    "        \"temperature\": 0.2, \"top_p\": 0.95\n",
    "    },\n",
    "    \"gemini-2.0-flash\": {\n",
    "        \"temperature\": 0.2, \"top_p\": 0.95\n",
    "    },\n",
    "    \"gemini-2.5-flash\": {\n",
    "        \"temperature\": 0.2, \"top_p\": 0.95\n",
    "    },\n",
    "    \"gemini-2.5-pro\": {\n",
    "        \"temperature\": 0.1, \"top_p\": 0.95\n",
    "    },\n",
    "    \"o1-mini\": {\n",
    "        \"temperature\": 0.3, \"top_p\": 1.0\n",
    "    },\n",
    "    \"microsoft/phi-4-reasoning-plus:free\": {\n",
    "        \"temperature\": 0.3, \"top_p\": 0.95\n",
    "    },\n",
    "    \"qwen/qwen3-30b-a3b:free\": {\n",
    "        \"temperature\": 0.3, \"top_p\": 0.95\n",
    "    },\n",
    "    \"google/gemini-2.5-flash-preview-05-20:thinking\": {\n",
    "        \"temperature\": 0.3, \"top_p\": 1.0\n",
    "    },\n",
    "    \"google/gemini-2.5-pro\": {\n",
    "        \"temperature\": 0.1, \"top_p\": 1.0\n",
    "    },\n",
    "    \"tngtech/deepseek-r1t-chimera:free\": {\n",
    "        \"temperature\": 0.3, \"top_p\": 0.95\n",
    "    },\n",
    "    \"anthropic/claude-sonnet-4\": {\n",
    "        \"temperature\": 0.2, \"top_p\": 0.95\n",
    "    },\n",
    "    \"anthropic/claude-3.5-haiku:beta\": {\n",
    "        \"temperature\": 0.2, \"top_p\": 0.95\n",
    "    },\n",
    "    \"gpt-4o\": {\n",
    "        \"temperature\": 0.2, \"top_p\": 0.95\n",
    "    },\n",
    "    \"gpt-4.1\": {\n",
    "        \"temperature\": 0.1, \"top_p\": 0.9  # This model ignores top_p\n",
    "    },\n",
    "    \"openai/gpt-5\": {\n",
    "        \"temperature\": 0.1, \"top_p\": 0.95\n",
    "    },\n",
    "    \"openai/gpt-4.1\": {\n",
    "        \"temperature\": 0.1, \"top_p\": 0.95\n",
    "    },\n",
    "    \"openai/gpt-oss-120b\": {\n",
    "        \"temperature\": 0.2, \"top_p\": 0.95\n",
    "    },\n",
    "    \"x-ai/grok-3-beta\": {\n",
    "        \"temperature\": 0.2, \"top_p\": 0.95\n",
    "    },\n",
    "    \"rekaai/reka-flash-3:free\": {\n",
    "        \"temperature\": 0.2, \"top_p\": 0.95\n",
    "    },\n",
    "    \"deepseek/deepseek-r1-distill-qwen-32b:free\": {\n",
    "        \"temperature\": 0.3, \"top_p\": 0.95\n",
    "    },\n",
    "    \"mistralai/mistral-large-2411\": {\n",
    "        \"temperature\": 0.0, \"top_p\": 0.95\n",
    "    },\n",
    "    \"deepseek/deepseek-chat-v3.1\": {\n",
    "        \"temperature\": 0.1, \"top_p\": 0.95\n",
    "    },\n",
    "    \"deepcogito/cogito-v2-preview-deepseek-671b\": {\n",
    "        \"temperature\": 0.1, \"top_p\": 0.95\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJDWFWeDzgX6",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Init models spec (only run one time for all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCbI2-49snQF"
   },
   "outputs": [],
   "source": [
    "models_spec={\n",
    "    \"llama3-70b-8192\": {\n",
    "        \"name\": \"llama3-70b-8192\",\n",
    "        \"api_host\": \"groq\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"llama-3.3-70b-versatile\":{\n",
    "        \"name\":\"llama-3.3-70b-versatile\",\n",
    "        \"api_host\": \"groq\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"meta-llama/llama-4-maverick-17b-128e-instruct\":{\n",
    "        \"name\": \"llama-4-maverick-17b-128e-instruct\",\n",
    "        \"api_host\": \"groq\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"meta-llama/llama-4-scout-17b-16e-instruct\":{\n",
    "        \"name\": \"llama-4-scout-17b-16e-instruct\",\n",
    "        \"api_host\": \"groq\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"qwen-qwq-32b\":{\n",
    "        \"name\": \"qwen-qwq-32b\",\n",
    "        \"api_host\": \"groq\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"gemini-1.5-flash\":{\n",
    "        \"name\": \"gemini-1.5-flash\",\n",
    "        \"api_host\": \"google\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"gemma-3-27b-it\":{\n",
    "        \"name\": \"gemma-3-27b-it\",\n",
    "        \"api_host\": \"google\",\n",
    "        \"json_format\": False,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"gemini-2.0-flash\":{\n",
    "        \"name\": \"gemini-2.0-flash\",\n",
    "        \"api_host\": \"google\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"gemini-2.5-flash-preview-04-17\":{\n",
    "        \"name\": \"gemini-2.5-flash-preview-04-17\",\n",
    "        \"api_host\": \"google\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"gemini-2.5-flash\":{\n",
    "        \"name\": \"gemini-2.5-flash\",\n",
    "        \"api_host\": \"google\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"gemini-2.5-pro\":{\n",
    "        \"name\": \"gemini-2.5-pro\",\n",
    "        \"api_host\": \"google\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"deepseek-r1-distill-llama-70b\":{\n",
    "        \"name\": \"deepseek-r1-distill-llama-70b\",\n",
    "        \"api_host\": \"groq\",\n",
    "        \"json_format\": False,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"gpt-3.5-turbo-0125\":{\n",
    "        \"name\": \"gpt-3.5-turbo-0125\",\n",
    "        \"api_host\": \"openai\",\n",
    "        \"json_format\": \"json_object\",\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"gpt-4.1-mini\":{\n",
    "        \"name\": \"gpt-4.1-mini\",\n",
    "        \"api_host\": \"openai\",\n",
    "        \"json_format\": \"json_object\",\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"gpt-4o\":{\n",
    "        \"name\": \"gpt-4o\",\n",
    "        \"api_host\": \"openai\",\n",
    "        \"json_format\": \"json_object\",\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"gpt-4.1\":{\n",
    "        \"name\": \"gpt-4.1\",\n",
    "        \"api_host\": \"openai\",\n",
    "        \"json_format\": \"json_object\",\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"openai/o1-preview\":{\n",
    "        \"name\": \"o1-preview\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"openai/gpt-5\": {\n",
    "        \"name\": \"gpt-5\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"openai/gpt-4.1\":{\n",
    "        \"name\": \"gpt-4.1\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"openai/gpt-oss-120b\":{\n",
    "        \"name\": \"gpt-oss-120b\",\n",
    "        \"api_host\": \"groq\",\n",
    "        \"json_format\": False,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"microsoft/phi-4-reasoning-plus:free\":{\n",
    "        \"name\": \"phi-4-reasoning-plus\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"google/gemini-2.5-flash-preview-05-20:thinking\":{\n",
    "        \"name\": \"gemini-2.5-flash-preview-05-20\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"google/gemini-2.5-pro\":{\n",
    "        \"name\": \"gemini-2.5-pro-preview\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"qwen/qwen3-30b-a3b:free\":{\n",
    "        \"name\": \"qwen3-30b-a3b\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"qwen/qwq-32b:free\":{\n",
    "        \"name\": \"qwen-qwq-32b\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"deepseek/deepseek-chat\":{\n",
    "        \"name\": \"deepseek-chat\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"deepseek/deepseek-r1-distill-qwen-32b:free\":{\n",
    "        \"name\": \"deepseek-r1-distill-qwen-32b\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    'tngtech/deepseek-r1t-chimera:free':{\n",
    "        \"name\": 'deepseek-r1t-chimera',\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"deepseek/deepseek-prover-v2:free\":{\n",
    "        \"name\": \"deepseek-prover-v2\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"deepseek/deepseek-prover-v2\":{\n",
    "        \"name\": \"deepseek-prover-v2\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"mistralai/mistral-small-3.1-24b-instruct:free\":{\n",
    "        \"name\": \"mistral-small-3.1-24b-instruct\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"anthropic/claude-sonnet-4\":{\n",
    "        \"name\": \"claude-sonnet-4\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"anthropic/claude-3.5-haiku:beta\":{\n",
    "        \"name\": \"claude-3.5-haiku\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"x-ai/grok-3-beta\":{\n",
    "        \"name\": \"grok-3-beta\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"rekaai/reka-flash-3:free\":{\n",
    "        \"name\": \"reka-flash-3\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": True,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"qwen/qwen3-32b\":{\n",
    "        \"name\": \"qwen3-32b\",\n",
    "        \"api_host\": \"groq\",\n",
    "        \"json_format\": False,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"mistralai/mistral-large-2411\":{\n",
    "        \"name\": \"mitral-large-2411\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": False,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"deepseek/deepseek-chat-v3.1\":{\n",
    "        \"name\": \"deepseek-chat-v3.1\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": False,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "    \"deepcogito/cogito-v2-preview-deepseek-671b\":{\n",
    "        \"name\": \"cogito-v2-671b\",\n",
    "        \"api_host\": \"openrouter\",\n",
    "        \"json_format\": False,\n",
    "        \"available\":True,\n",
    "        \"valid_table_generated\": 0,\n",
    "        \"table_generated\": 0,\n",
    "        \"calls\": 0,\n",
    "        \"valid_out_format\": 0,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "with open(\"drive/MyDrive/TableQA/models_spec.json\", \"w\") as f:\n",
    "    json.dump(models_spec, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rr4QrQ0o9Sl_"
   },
   "outputs": [],
   "source": [
    "with open(\"drive/MyDrive/TableQA/time.json\", \"w\") as f:\n",
    "    time_file = {'start_time': datetime.now().isoformat()}\n",
    "    json.dump(time_file, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYjJjndO0mmR"
   },
   "source": [
    "#### Load models spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mZ6svT8lyx3H"
   },
   "outputs": [],
   "source": [
    "with open(\"drive/MyDrive/TableQA/models_spec.json\", \"r\") as f:\n",
    "    models_spec = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "OsB_9dGwiUvT"
   },
   "outputs": [],
   "source": [
    "qa_evaluators=[\"gemini-2.5-pro\", \"gpt-4.1\", \"mistralai/mistral-large-2411\", \"deepseek/deepseek-chat-v3.1\", \"deepcogito/cogito-v2-preview-deepseek-671b\"]\n",
    "\n",
    "qa_generators=[\"gemini-2.0-flash\", \"qwen/qwen3-32b\", \"openai/gpt-5\", \"openai/gpt-oss-120b\", \"qwen/qwen3-30b-a3b:free\",\n",
    "               \"gemini-2.5-flash\", \"anthropic/claude-sonnet-4\", \"x-ai/grok-3-beta\", 'gemini-2.5-pro']\n",
    "\n",
    "\"\"\"\n",
    "table_generators=[\"gemini-2.0-flash\", \"qwen/qwen3-30b-a3b:free\", 'tngtech/deepseek-r1t-chimera:free', \"anthropic/claude-3.5-haiku:beta\", \"rekaai/reka-flash-3:free\", \"deepseek/deepseek-r1-distill-qwen-32b:free\",\n",
    "                 ]\n",
    "\"\"\"\n",
    "\n",
    "table_generators=[\"gemini-2.5-flash\", \"gemini-2.5-pro\", \"anthropic/claude-sonnet-4\", \"gpt-4.1\", \"x-ai/grok-3-beta\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "cjew5zA162vP"
   },
   "outputs": [],
   "source": [
    "api_keys = {'groq': groq_key, 'google': google_key, 'openai': openai_key, 'openrouter': openrouter_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "uvzTssBTJHwm"
   },
   "outputs": [],
   "source": [
    "config = {'TABLE_INSTRUCT_LATEX': TABLE_INSTRUCT_LATEX,\n",
    "          'QA_INSTRUCT': QA_INSTRUCT,\n",
    "          'QA_EVAL_INSTRUCT': QA_EVAL_INSTRUCT,\n",
    "          'generation_settings': generation_settings,\n",
    "          'models_spec': models_spec,\n",
    "          'qa_evaluators': qa_evaluators,\n",
    "          'qa_generators': qa_generators,\n",
    "          'table_generators': table_generators,\n",
    "          'models_spec_path': \"drive/MyDrive/TableQA/models_spec.json\",\n",
    "          'time_path': \"drive/MyDrive/TableQA/time.json\",\n",
    "          'latex_output_dir': \"drive/MyDrive/TableQA/dataset/Latex\",\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqGjS7V4ywt4"
   },
   "source": [
    "### Visual-TableQA class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "m2jbv-eZ40j7"
   },
   "outputs": [],
   "source": [
    "# TableQA v2 class\n",
    "class TableQA_v2:\n",
    "  \"\"\" Creates a Synthetic Dataset on Tables for QA Reasoning and Recognition Tasks.\"\"\"\n",
    "  def __init__(self, api_keys, config):\n",
    "      self.config = config\n",
    "\n",
    "      # Setting up the LLM hosts\n",
    "      self.groq_client = Groq(api_key=api_keys['groq'])\n",
    "      genai.configure(api_key=api_keys['google'])\n",
    "      self.openai_client = OpenAI(api_key=api_keys['openai'])\n",
    "      self.openrouter_key = api_keys['openrouter']\n",
    "      self.update_models_usage()\n",
    "\n",
    "  def update_models_usage(self):\n",
    "      if update_start_time(self.config['time_path']):\n",
    "          # reset the counters\n",
    "          for model in self.config['models_spec']:\n",
    "              config['models_spec'][model]['available'] = True\n",
    "\n",
    "          print(\"Models usage reinitialized\")\n",
    "\n",
    "      # save the models usage\n",
    "      with open(self.config['models_spec_path'], \"w\") as f:\n",
    "              json.dump(self.config['models_spec'], f, indent=2)\n",
    "\n",
    "\n",
    "  def model_selector(self, task):\n",
    "      model_list = 'table_generators' if task=='table' else 'qa_generators'\n",
    "      weights = [int(self.config['models_spec'][model]['available']) for model in self.config[model_list]]\n",
    "      model_id= random.choices(range(len(weights)), weights=weights, k=1)[0]\n",
    "      model_name = self.config[model_list][model_id]\n",
    "      print(\"----------------------selected model \", model_name)\n",
    "      return model_name\n",
    "\n",
    "  def get_settings(self, model_name):\n",
    "      if model_name in self.config['generation_settings']:\n",
    "          temperature = self.config['generation_settings'][model_name]['temperature']\n",
    "          top_p = self.config['generation_settings'][model_name]['top_p']\n",
    "      else:\n",
    "          print(model_name, \" does not have generation settings\")\n",
    "          temperature, top_p = 0.2, 0.9\n",
    "      return temperature, top_p\n",
    "\n",
    "  def call_llm(self, model_name, prompt, max_tokens):\n",
    "      json_format = self.config['models_spec'][model_name]['json_format']\n",
    "      temperature, top_p = self.get_settings(model_name)\n",
    "\n",
    "      api_host = self.config['models_spec'][model_name]['api_host']\n",
    "      messages = prompt if api_host == 'google' else [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "      if api_host.lower() == \"groq\":\n",
    "          completion = self.groq_client.chat.completions.create(model=model_name,messages=messages,temperature=temperature, top_p= top_p,\n",
    "              max_completion_tokens=20000,stream=False,\n",
    "              reasoning_format='hidden' if model_name in [\"deepseek-r1-distill-llama-70b\", \"qwen/qwen3-32b\", \"openai/gpt-oss-120b\"] else None,\n",
    "              response_format={\"type\": \"json_object\"} if json_format else None,\n",
    "              reasoning_effort='high' if model_name==\"openai/gpt-oss-120b\" else None,\n",
    "          )\n",
    "          response = completion.choices[0].message.content\n",
    "          if completion.choices[0].finish_reason == \"stop\":\n",
    "              return response if json_format else extract_json_blocks(response)\n",
    "          else:\n",
    "              print(completion.choices[0].finish_reason)\n",
    "              raise Exception(f\"API call ended before task. Reason: {completion.choices[0].finish_reason}\")\n",
    "\n",
    "      elif api_host.lower() == \"google\":\n",
    "          model = genai.GenerativeModel(model_name)\n",
    "          generation_config = {\n",
    "                                \"max_output_tokens\": max_tokens,\n",
    "                                \"temperature\": temperature,\n",
    "                                \"top_p\": top_p,\n",
    "                                \"response_mime_type\": \"application/json\" if json_format else None,\n",
    "                                }\n",
    "          response = model.generate_content(contents=messages, generation_config=generation_config)\n",
    "          if (response.prompt_feedback is None or\n",
    "                response.prompt_feedback.block_reason.name == \"BLOCK_REASON_UNSPECIFIED\"):\n",
    "              return response.text if json_format else extract_json_blocks(response.text)\n",
    "          else:\n",
    "              print(response.prompt_feedback.block_reason.name)\n",
    "              raise Exception(f\"API call ended before task. Reason: {response.prompt_feedback.block_reason.name}\")\n",
    "\n",
    "      elif api_host.lower() == \"openai\":\n",
    "          completion = self.openai_client.chat.completions.create(model=model_name,messages=messages,temperature=temperature,\n",
    "                  top_p= top_p, max_completion_tokens=max_tokens,response_format= {\"type\": json_format})\n",
    "\n",
    "          if completion.choices[0].finish_reason == \"stop\":\n",
    "              return completion.choices[0].message.content\n",
    "          else:\n",
    "              print(completion.choices[0].finish_reason)\n",
    "              raise Exception(f\"API call ended before task. Reason: {completion.choices[0].finish_reason}\")\n",
    "\n",
    "      elif api_host.lower() == \"openrouter\":\n",
    "          if model_name == \"deepseek/deepseek-chat-v3.1\":\n",
    "                args = {\"model\": model_name, \"messages\": messages, \"temperature\": temperature, \"top_p\": top_p,\n",
    "                        \"reasoning\": {\"effort\": \"high\", \"exclude\": False}}\n",
    "          else:\n",
    "              args = {\"model\": model_name, \"messages\": messages, \"temperature\": temperature, \"top_p\": top_p}\n",
    "\n",
    "          if json_format:\n",
    "              args[\"response_format\"]= {\"type\": \"json_object\"}\n",
    "              \n",
    "          response = requests.post(url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "                                   headers={\"Authorization\": f\"Bearer {self.openrouter_key}\", \"Content-Type\": \"application/json\"},\n",
    "                                   data=json.dumps(args))\n",
    "          response = response.json()\n",
    "          if response['choices'][0]['finish_reason'] == \"stop\":\n",
    "              return extract_json_blocks(response['choices'][0]['message']['content'])\n",
    "          else:\n",
    "              print(response['choices'][0]['finish_reason'])\n",
    "              raise Exception(f\"API call ended before task. Reason: {response['choices'][0]['finish_reason']}\")\n",
    "\n",
    "      else:\n",
    "        raise Exception(\"Please choose api cloud host from 'google', 'groq', 'openai' and 'openrouter'.\")\n",
    "\n",
    "\n",
    "  def safe_call_llm(self, model_name, prompt, max_tokens=5000):\n",
    "      #time.sleep(random.uniform(1, 3))\n",
    "      try:\n",
    "          return self.call_llm(model_name, prompt, max_tokens), None\n",
    "      except Exception as e:\n",
    "          # Handle API calls limits\n",
    "          msg = read_Exception(e).lower()\n",
    "          if any(keyword in msg for keyword in [\"quota\", \"billing\", \"insufficient\", \"resource exhausted\", \"429\", \"402\"]):\n",
    "              self.config['models_spec'][model_name]['available'] = False\n",
    "              print(model_name, \"has reached its limit\")\n",
    "          elif 'ended' in msg:\n",
    "              print(model_name, \"needs more tokens to think\")\n",
    "          return None, msg\n",
    "\n",
    "\n",
    "  def safe_json_loads(self, json_text):\n",
    "      try:\n",
    "          return json.loads(json_text), None\n",
    "      except Exception as e:\n",
    "          msg = str(e).lower()\n",
    "          return None, msg\n",
    "\n",
    "  def generate_synthetic_table(self, table_inspo, topic_inspo):\n",
    "\n",
    "      table_instruct = self.config['TABLE_INSTRUCT_LATEX']\n",
    "      # Prepare the query to generate the table\n",
    "      query = f\"{table_instruct}\\n\\nTable example:\\n{table_inspo}\" if table_inspo is not None else table_instruct\n",
    "      query = f\"{query}\\n\\nInspiration topics: {topic_inspo}\" if topic_inspo is not None else query\n",
    "      query += \"\\n\\nGenerate tables\"\n",
    "\n",
    "      # Generate tables\n",
    "      call_status = \"failed\"\n",
    "      while call_status is not None:\n",
    "          model = self.model_selector('table') # Select model\n",
    "          table, call_status = self.safe_call_llm(model_name=model, prompt=query)\n",
    "      print(\"Table model: \", model)\n",
    "\n",
    "      table = extract_tables_from_text(table)\n",
    "      return table, model\n",
    "\n",
    "  def generate_reasoning_qa(self, table, instruction):\n",
    "      # Prepare the query\n",
    "      query = f\"{instruction}\\n\\nTable:\\n{table}\\n\\n\"\n",
    "\n",
    "      # Generate the questions and answers for reasoning capabilities\n",
    "      call_status = \"failed\"\n",
    "      while call_status is not None:\n",
    "          model = self.model_selector('qa') # Select model\n",
    "          qa_pairs, call_status = self.safe_call_llm(model_name=model, prompt=query)\n",
    "      print(\"QA model: \", model, '\\n\\n')\n",
    "      qa_pairs, json_load_status = self.safe_json_loads(qa_pairs)\n",
    "      self.config['models_spec'][model]['calls'] += 1\n",
    "      self.config['models_spec'][model]['valid_out_format'] += 1 if json_load_status is None else 0\n",
    "      return qa_pairs, model, json_load_status\n",
    "\n",
    "  def generate_dataset_rows(self, table, table_name):\n",
    "\n",
    "      qa_instruction = self.config['QA_INSTRUCT']\n",
    "\n",
    "      # Generate Resoning Q&A\n",
    "      json_load_status = \"failed\"\n",
    "      while json_load_status is not None:\n",
    "          qa_pairs, model, json_load_status = self.generate_reasoning_qa(table=table, instruction=qa_instruction)\n",
    "\n",
    "      dataset_rows = []\n",
    "      n_qa_pairs=min(len(qa_pairs['questions']), len(qa_pairs['answers']))\n",
    "      if len(qa_pairs['questions']) != len(qa_pairs['answers']):\n",
    "          print(f\"Mismatch in QA pairs. {len(qa_pairs['questions'])} questions for {len(qa_pairs['answers'])} answers.\")\n",
    "      for k in range(n_qa_pairs):\n",
    "          dataset_row = {\n",
    "              'table_image': table_name,\n",
    "              'question': qa_pairs['questions'][k],\n",
    "              'answer': qa_pairs['answers'][k],\n",
    "              'model_name': model,\n",
    "          }\n",
    "          dataset_rows.append(dataset_row)\n",
    "\n",
    "      return dataset_rows\n",
    "\n",
    "  def evaluate_single_jury(self, model, qa_eval_prompt):\n",
    "      qa_eval_prompt=qa_eval_prompt.replace(\"JSON_mention\", \"JSON\") if model=='gpt-4.1' else qa_eval_prompt.replace(\"JSON_mention\", \"\\n\")\n",
    "      for attempt in range(3):\n",
    "          decision, call_status = self.safe_call_llm(model_name=model, prompt=qa_eval_prompt)\n",
    "          if call_status is None:\n",
    "              decision = json.loads(decision)\n",
    "              verdict, reason = decision['decision'][0], decision['decision'][1]\n",
    "              return {\"verdict\": verdict, \"reason\": reason, \"success\": True}\n",
    "          # Exceeded retries, skip this jury\n",
    "          return {\"verdict\": 0, \"success\": False, \"error\": call_status}\n",
    "\n",
    "  def evaluate_qa_pair(self, qa_eval_prompt):\n",
    "      evaluations = {model: self.evaluate_single_jury(model, qa_eval_prompt) for model in self.config['qa_evaluators']}\n",
    "        \n",
    "      # Keep only successful juries\n",
    "      successful = [eval for _,eval in evaluations.items() if eval.get(\"success\")]\n",
    "      verdicts = [eval[\"verdict\"] for eval in successful]\n",
    "      if not verdicts:\n",
    "          majority = None\n",
    "          print(\"no verdict\")\n",
    "      elif verdicts.count(1) > verdicts.count(0):\n",
    "          majority = 1\n",
    "      elif verdicts.count(1) < verdicts.count(0):\n",
    "          majority = 0\n",
    "      else:\n",
    "          majority = evaluations[\"gpt-4.1\"][\"verdict\"]\n",
    "            \n",
    "      confidence = 0.0 if not verdicts else verdicts.count(majority) / len(verdicts)\n",
    "\n",
    "      print(f\"Verdict: {majority}, Confidence: {confidence}, Successful_juries: {len(successful)}\")\n",
    "      return {\n",
    "          \"evaluations\": evaluations,\n",
    "          \"final_verdict\": majority,\n",
    "          \"confidence\": confidence,\n",
    "          \"successful_juries\": len(successful),\n",
    "          \"total_juries\": len(evaluations)\n",
    "        }\n",
    "\n",
    "\n",
    "  def generate_dataset(self, table_inspo=None, topic_inspo=None, show_img=False):\n",
    "      # Generate synthetic table inspired by table_inspo\n",
    "      try:\n",
    "          synthetic_tables, table_model = self.generate_synthetic_table(table_inspo=table_inspo, topic_inspo=topic_inspo)\n",
    "      except Exception as e:\n",
    "          print(type(e).__name__)\n",
    "          return [], None\n",
    "\n",
    "      dataset = []\n",
    "      for (k,v) in synthetic_tables.items():\n",
    "          self.config['models_spec'][table_model]['table_generated'] += 1\n",
    "          try:\n",
    "              # Save table as image\n",
    "              table_name = generate_unique_filename(prefix=self.config['models_spec'][table_model]['name'])\n",
    "              save_latex_table_as_image(table=v, table_name=table_name, output_dir=self.config['latex_output_dir'], show_img=show_img)\n",
    "              self.config['models_spec'][table_model]['valid_table_generated'] += 1\n",
    "\n",
    "              # Generate dataset rows\n",
    "              dataset_rows = self.generate_dataset_rows(table=v, table_name=table_name)\n",
    "\n",
    "              # Evaluate QA pairs using LLMs as juries\n",
    "              qa_eval_prompt = self.config['QA_EVAL_INSTRUCT'] + f\"Table: {v}\\n\" + f\"Topics: {topic_inspo}\\n\"\n",
    "              for row in dataset_rows:\n",
    "                  evaluations = self.evaluate_qa_pair(f\"{qa_eval_prompt}Question: {row['question']}\\nAnswer: {row['answer']}\\n\")\n",
    "                  row['decision'] = evaluations['final_verdict']\n",
    "                  row['detailed_evaluations'] = evaluations\n",
    "\n",
    "              dataset.extend(dataset_rows)\n",
    "          except Exception as e:\n",
    "              print(type(e).__name__)\n",
    "\n",
    "      self.update_models_usage()\n",
    "\n",
    "      return dataset, synthetic_tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwhxR7YgXRMK"
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0E_FjHp09SmC"
   },
   "source": [
    "##### Table inspos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BD_lbtIi9SmC"
   },
   "outputs": [],
   "source": [
    "latex_table_inspo = []\n",
    "\n",
    "folder_path = \"drive/MyDrive/TableQA/dataset/Latex/inspo_3\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".tex\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                table = f.read()\n",
    "                latex_table_inspo.append(table)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lF_ipRpB7xTC"
   },
   "source": [
    "##### generation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qmH8lyXxXVxo",
    "outputId": "af6d3fdd-e599-4805-e81c-2991157fcef4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from drive.MyDrive.TableQA.samples.synthetic_table_topics import topics\n",
    "\n",
    "start_idx = 780\n",
    "counter = 0\n",
    "dataset = []\n",
    "\n",
    "config['latex_output_dir']= \"drive/MyDrive/TableQA/dataset/Latex/table_topic_4\"\n",
    "\n",
    "tqa = TableQA_v2(api_keys=api_keys, config=config)\n",
    "# Save everything\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"data_{timestamp}.jsonl\"\n",
    "output_path = os.path.join(config['latex_output_dir'], filename)\n",
    "\n",
    "while counter <100:\n",
    "    print('\\nIteration : ', counter)\n",
    "    topic_inspo = topics[3*(counter+start_idx): 3*(counter+start_idx)+3]\n",
    "    topic_inspo = \", \".join(topic_inspo)\n",
    "    table_inspo = random.sample(latex_table_inspo, 1)[0]\n",
    "    output, tables = tqa.generate_dataset(table_inspo=table_inspo, topic_inspo=topic_inspo, show_img=False)\n",
    "    dataset.extend(output)\n",
    "    if len(output)!=0:\n",
    "        with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            for entry in output:\n",
    "                f.write(json.dumps(entry) + \"\\n\")\n",
    "    counter += 1\n",
    "\n",
    "print('\\n\\n-------------valid_table_generated vs table_generated----------------\\n')\n",
    "pprint({k:(tqa.config['models_spec'][k]['valid_table_generated'], tqa.config['models_spec'][k]['table_generated']) for k in tqa.config['table_generators']})\n",
    "print('\\n\\n-------------valid_output_format vs calls----------------\\n')\n",
    "pprint({k:(tqa.config['models_spec'][k]['valid_out_format'], tqa.config['models_spec'][k]['calls']) for k in tqa.config['qa_generators']})\n",
    "print('\\n\\n-------------availability----------------\\n')\n",
    "pprint({k:tqa.config['models_spec'][k]['available'] for k in tqa.config['qa_generators']})\n",
    "\n",
    "\n",
    "print(f\"Saved {len(dataset)} datapoints to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "dJDWFWeDzgX6",
    "_Zl6zXJ8fv52"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
