{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1648798-37fa-400c-86d1-4e1891742532",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab9b94b-df1e-4a2d-be31-d3ade2b15fd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.6.0 torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3281e68-6ea0-4936-a044-59f743a28012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0+cu124torch2.6-cp310-cp310-linux_x86_64.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066dd584-f921-452d-9864-ae1bfc021bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U trl>=0.9.6 transformers>=4.42 peft>=0.12.0 accelerate>=0.33.0 bitsandbytes>=0.43.3 datasets>=2.18 qwen-vl-utils pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e630b-81f7-4bb5-97e7-c4fce3e39baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e0f2c-79e7-4940-a526-bcd60d76a108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4abeec4d-af77-4b3a-acb4-bc4bc86b105c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "from transformers import (AutoProcessor, AutoTokenizer, BitsAndBytesConfig, EarlyStoppingCallback, LlavaNextForConditionalGeneration, LlavaNextProcessor)\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from PIL import Image, ImageOps\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f12aea0-eba4-49eb-b563-896600eda737",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768eec0b-a4f3-49c5-908d-f5df4a74080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Config ---------\n",
    "MODEL_ID = \"llava-hf/llama3-llava-next-8b-hf\" \n",
    "DATASET_REPO = \"AI-4-Everyone/Visual-TableQA\"\n",
    "OUTPUT_DIR   = \"llava-hf-sft-lora-tableqa\"\n",
    "\n",
    "# --------- Load data ---------\n",
    "ds = load_dataset(DATASET_REPO)\n",
    "\n",
    "train = ds.get(\"train\")\n",
    "\n",
    "evald = ds.get(\"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d362b6-ea5d-41fc-93b2-6dd3ffd1c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from charts and diagrams images.\n",
    "Answer the questions strictly from the image, with clear, rigorous step-by-step justification. Stay concise, but include all reasoning thatâ€™s relevant.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81b9c7f5-a84d-4137-a40b-8bcf6985ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pil(img):\n",
    "    return img if isinstance(img, Image.Image) else Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46285959-8a13-4932-9763-32df0dcc7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pil(img):\n",
    "    return img if isinstance(img, Image.Image) else Image.fromarray(img)\n",
    "\n",
    "def build_messages(sample):\n",
    "    gt=sample[\"answer\"]\n",
    "    system= {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]}\n",
    "    user = {\"role\": \"user\", \"content\": [{\"type\":\"image\", \"image\": to_pil(sample['image'])}, {\"type\":\"text\",\"text\": sample['question']}]}\n",
    "    asst = {\"role\": \"assistant\", \"content\": [{\"type\":\"text\",\"text\": gt}]}\n",
    "    return [system, user], [system, user, asst]\n",
    "\n",
    "def collate_fn(examples):\n",
    "    msgs = [build_messages(example)[1] for example in examples]\n",
    "    texts = [processor.apply_chat_template(m, tokenize=False) for m in msgs]\n",
    "\n",
    "    image_inputs, _ = process_vision_info(msgs)\n",
    "    batch = processor(\n",
    "        text=texts,\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=False,\n",
    "        max_length=None,\n",
    "    )\n",
    "\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    # mask image tokens\n",
    "    image_token_ids = []\n",
    "    for tok in [\"<image>\", \"<im_patch>\", \"<im_start>\", \"<im_end>\"]:\n",
    "        tid = processor.tokenizer.convert_tokens_to_ids(tok)\n",
    "        if tid != processor.tokenizer.unk_token_id and tid is not None:\n",
    "            image_token_ids.append(tid)\n",
    "    if image_token_ids:\n",
    "        mask = torch.isin(labels, torch.tensor(image_token_ids, device=labels.device))\n",
    "        labels[mask] = -100\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f032867e-f15d-40ff-8bad-10d3a22cf3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_bf16 = torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e407dc-3dcb-4631-9fd8-0802051b9da7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ca503-1cb6-432a-ae7a-bc44d23c6872",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.pretraining_tp = 1\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "771af569-2a12-4e73-8f7c-f6dc90b1110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- LoRA config (r=16, alpha=8) ---------\n",
    "TARGETS = \"all-linear\"\n",
    "\n",
    "r, lora_alpha = 16, 8\n",
    "\n",
    "peft_cfg = LoraConfig(r=r, lora_alpha=lora_alpha, lora_dropout=0.05, bias=\"none\",\n",
    "                      target_modules=TARGETS, task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d396b84-5278-4fbc-a229-64afd27e42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PEFT model adaptation\n",
    "peft_model = get_peft_model(model, peft_cfg)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "131a51bc-89ad-4768-9487-0eed001508e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- SFT training args  ---------\n",
    "args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "\n",
    "    # dataset-sized schedule\n",
    "    num_train_epochs=1,         \n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    # stability & speed\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    bf16=use_bf16,\n",
    "    fp16=not use_bf16,\n",
    "    tf32=True,\n",
    "\n",
    "    # optimization\n",
    "    max_grad_norm=0.5,\n",
    "    learning_rate=2e-5,           # LoRA-friendly; fits higher capacity\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    optim=\"adamw_torch_fused\",      \n",
    "    adam_beta1=0.9, adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    weight_decay=0.1, #0.0,    \n",
    "    \n",
    "    # logging/eval/save: keep it simple on small data\n",
    "    logging_steps=40,\n",
    "    eval_strategy=\"steps\", \n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100, \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    dataset_text_field=\"\",  #   # <- stops TRL from looking for \"text\"\n",
    ")\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "803ea79a-7a0d-493c-82dc-4cb0f601cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=evald,\n",
    "    data_collator=collate_fn,\n",
    "    #peft_config=peft_cfg,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95853f14-f3fa-4db9-a3ee-24962744e914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92b8c8-c0f6-47a5-830f-b9baa782d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(OUTPUT_DIR)      # saves the adapters\n",
    "processor.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed61ac9c-bd7d-4fc1-a57b-2d51ce2b93c8",
   "metadata": {},
   "source": [
    "#### Resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca46b78-9d8c-45f2-9fb0-3caab1dc7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf620d9e-53ce-45b2-8f08-04ccd87eef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(OUTPUT_DIR)      # saves the adapters\n",
    "processor.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb9029-23fe-4111-bbbe-d99e4535ca98",
   "metadata": {},
   "source": [
    "## Final Model Loading and Inferance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0faeef34-d044-4985-9bae-f50220fa5441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c28e17d-ea10-4251-af90-549c98e1c650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 5 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:59<00:00, 11.94s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "adapter_path = OUTPUT_DIR\n",
    "model.load_adapter(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b5ef6de-7470-4d0b-945e-17ddf08be798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_sample(model, sample, max_new_tokens=5000, device=\"cuda\"):\n",
    "    text_input = processor.apply_chat_template(build_messages(sample)[0], tokenize=False, add_generation_prompt=True)\n",
    "    # Process the visual input from the sample\n",
    "    image_inputs, _ = process_vision_info(build_messages(sample)[0])\n",
    "    # Prepare the inputs for the model\n",
    "    model_inputs = processor(text=[text_input], images=image_inputs, return_tensors=\"pt\").to(device)  # Move inputs to the specified device\n",
    "\n",
    "    gen_kwargs = {\"max_new_tokens\":max_new_tokens, \"do_sample\":False, \"pad_token_id\": processor.tokenizer.pad_token_id, \n",
    "                  \"eos_token_id\": processor.tokenizer.eos_token_id}\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(**model_inputs, **gen_kwargs)\n",
    "\n",
    "    # Decode only the generated part (exclude input tokens)\n",
    "    input_token_len = model_inputs[\"input_ids\"].shape[1]\n",
    "    generated_ids = output_ids[:, input_token_len:]\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = processor.decode(\n",
    "        generated_ids[0], \n",
    "        skip_special_tokens=True, \n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    \n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4835e3b3-cf1c-4f93-9297-88287bc1aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate_text_from_sample(model, processor, train[0])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858e290-5134-46c9-bd13-c9e2d9c609f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
