{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b1648798-37fa-400c-86d1-4e1891742532",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "b1648798-37fa-400c-86d1-4e1891742532"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ab9b94b-df1e-4a2d-be31-d3ade2b15fd2",
      "metadata": {
        "id": "7ab9b94b-df1e-4a2d-be31-d3ade2b15fd2"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.6.0 torchvision torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3281e68-6ea0-4936-a044-59f743a28012",
      "metadata": {
        "id": "c3281e68-6ea0-4936-a044-59f743a28012"
      },
      "outputs": [],
      "source": [
        "!pip install https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0+cu124torch2.6-cp310-cp310-linux_x86_64.whl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "066dd584-f921-452d-9864-ae1bfc021bc6",
      "metadata": {
        "id": "066dd584-f921-452d-9864-ae1bfc021bc6"
      },
      "outputs": [],
      "source": [
        "!pip install -U trl>=0.9.6 transformers>=4.42 peft>=0.12.0 accelerate>=0.33.0 bitsandbytes>=0.43.3 datasets>=2.18 trl>=0.9.6 qwen-vl-utils pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d60e0f2c-79e7-4940-a526-bcd60d76a108",
      "metadata": {
        "scrolled": true,
        "id": "d60e0f2c-79e7-4940-a526-bcd60d76a108"
      },
      "outputs": [],
      "source": [
        "!env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4abeec4d-af77-4b3a-acb4-bc4bc86b105c",
      "metadata": {
        "id": "4abeec4d-af77-4b3a-acb4-bc4bc86b105c"
      },
      "outputs": [],
      "source": [
        "import os, torch\n",
        "from datasets import load_dataset\n",
        "import torch.nn.functional as F\n",
        "from transformers import (AutoProcessor, AutoTokenizer, EarlyStoppingCallback, LlavaNextForConditionalGeneration, LlavaNextProcessor)\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from PIL import Image, ImageOps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f12aea0-eba4-49eb-b563-896600eda737",
      "metadata": {
        "id": "9f12aea0-eba4-49eb-b563-896600eda737"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "768eec0b-a4f3-49c5-908d-f5df4a74080b",
      "metadata": {
        "id": "768eec0b-a4f3-49c5-908d-f5df4a74080b",
        "outputId": "a8e18d74-a8eb-4737-c936-db1aeda53ef6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 100%|██████████| 3617/3617 [00:13<00:00, 258.38 examples/s]\n",
            "Generating validation split: 100%|██████████| 779/779 [00:00<00:00, 1027.62 examples/s]\n",
            "Generating test split: 100%|██████████| 769/769 [00:00<00:00, 1245.79 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# --------- Config ---------\n",
        "MODEL_ID = \"llava-hf/llama3-llava-next-8b-hf\"  # LLaVA-NeXT-Llama3-8B (HF port)\n",
        "DATASET_REPO = \"AI-4-Everyone/Visual-TableQA\"\n",
        "OUTPUT_DIR   = \"llava-hf-sft-lora-tableqa\"\n",
        "\n",
        "# --------- Load data ---------\n",
        "ds = load_dataset(DATASET_REPO)\n",
        "\n",
        "train = ds.get(\"train\")\n",
        "evald = ds.get(\"validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f032867e-f15d-40ff-8bad-10d3a22cf3cd",
      "metadata": {
        "id": "f032867e-f15d-40ff-8bad-10d3a22cf3cd"
      },
      "outputs": [],
      "source": [
        "use_bf16 = torch.cuda.is_bf16_supported()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a1ca503-1cb6-432a-ae7a-bc44d23c6872",
      "metadata": {
        "id": "0a1ca503-1cb6-432a-ae7a-bc44d23c6872",
        "outputId": "689518b8-30c6-4e00-9d80-723b7500cb4e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 5 files: 100%|██████████| 5/5 [00:59<00:00, 11.95s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 5/5 [00:05<00:00,  1.20s/it]\n",
            "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
            "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
          ]
        }
      ],
      "source": [
        "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "model.config.use_cache = False\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.pretraining_tp = 1\n",
        "processor.tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a405846-5d38-4a6c-907f-c5b110209bf9",
      "metadata": {
        "id": "3a405846-5d38-4a6c-907f-c5b110209bf9"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from charts and diagrams images.\n",
        "Answer the questions strictly from the image, with clear, rigorous step-by-step justification. Stay concise, but include all reasoning that’s relevant.\"\"\"\n",
        "\n",
        "\n",
        "from typing import List, Dict\n",
        "\n",
        "def build_messages(q:str, a:str):\n",
        "    user = {\"role\": \"user\", \"content\": [{\"type\":\"image\"}, {\"type\":\"text\",\"text\": q}]}\n",
        "    asst = {\"role\": \"assistant\", \"content\": [{\"type\":\"text\",\"text\": a}]}\n",
        "    return [user], [user, asst]\n",
        "\n",
        "LONG_EDGE = 1024\n",
        "\n",
        "def to_pil(img):\n",
        "    return img if isinstance(img, Image.Image) else Image.fromarray(img)\n",
        "\n",
        "def clamp_long_edge(img, longest=LONG_EDGE):\n",
        "    img = to_pil(img)\n",
        "    # preserve aspect ratio; bicubic keeps text/lines readable\n",
        "    return ImageOps.contain(img, (longest, longest), Image.Resampling.BICUBIC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e9e28bd-f877-4209-b798-b883c88d3452",
      "metadata": {
        "id": "9e9e28bd-f877-4209-b798-b883c88d3452"
      },
      "outputs": [],
      "source": [
        "def collate_fn(examples: List[Dict]):\n",
        "    images, prompts_user, prompts_full = [], [], []\n",
        "    for ex in examples:\n",
        "        img = clamp_long_edge(ex[\"image\"])  # PIL.Image from datasets\n",
        "        q, a = ex[\"question\"], ex[\"answer\"]\n",
        "        user_msg, full_msg = build_messages(q, a)\n",
        "        images.append(img)\n",
        "        prompts_user.append(processor.apply_chat_template(user_msg, add_generation_prompt=True, tokenize=False))\n",
        "        prompts_full.append(processor.apply_chat_template(full_msg, add_generation_prompt=False, tokenize=False))\n",
        "\n",
        "    # Batched tokenize + image processing (handles image token expansion & padding)\n",
        "    batch_user = processor(images, prompts_user, return_tensors=\"pt\", padding=True)\n",
        "    batch_full = processor(images, prompts_full, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    input_ids = batch_full[\"input_ids\"]\n",
        "    attention_mask = batch_full[\"attention_mask\"]\n",
        "    pixel_values = batch_full[\"pixel_values\"]\n",
        "\n",
        "    # Compute per-sample offset = length of user-only sequence (including expanded image tokens)\n",
        "    user_lengths = batch_user[\"attention_mask\"].sum(dim=1)\n",
        "\n",
        "    # Build labels: supervise only assistant tokens\n",
        "    labels = input_ids.clone()\n",
        "    for i, L in enumerate(user_lengths.tolist()):\n",
        "        labels[i, :int(L)] = -100  # ignore user part\n",
        "\n",
        "    # Create batch dict with all required fields\n",
        "    batch_dict = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "    # Add image_sizes if present in the batch (required for LlavaNext)\n",
        "    if \"image_sizes\" in batch_full:\n",
        "        batch_dict[\"image_sizes\"] = batch_full[\"image_sizes\"]\n",
        "    else:\n",
        "        # Fallback: create image_sizes from actual image dimensions\n",
        "        image_sizes = []\n",
        "        for img in images:\n",
        "            if hasattr(img, 'size'):\n",
        "                image_sizes.append(list(img.size))  # PIL Image size is (width, height)\n",
        "            else:\n",
        "                image_sizes.append([LONG_EDGE, LONG_EDGE])  # fallback size\n",
        "        batch_dict[\"image_sizes\"] = torch.tensor(image_sizes)\n",
        "\n",
        "    return batch_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "771af569-2a12-4e73-8f7c-f6dc90b1110a",
      "metadata": {
        "id": "771af569-2a12-4e73-8f7c-f6dc90b1110a"
      },
      "outputs": [],
      "source": [
        "# --------- LoRA config (r=16, alpha=8) ---------\n",
        "\n",
        "TARGETS = \"all-linear\"\n",
        "r, lora_alpha = 16, 8\n",
        "peft_cfg = LoraConfig(r=r, lora_alpha=lora_alpha, lora_dropout=0.05, bias=\"none\",\n",
        "                      target_modules=TARGETS, task_type=\"CAUSAL_LM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d396b84-5278-4fbc-a229-64afd27e42d5",
      "metadata": {
        "id": "9d396b84-5278-4fbc-a229-64afd27e42d5",
        "outputId": "088e771d-d912-4c06-a346-20f7b0d1d023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 51,824,640 || all params: 8,343,991,296 || trainable%: 0.6211\n"
          ]
        }
      ],
      "source": [
        "# Apply PEFT model adaptation\n",
        "\n",
        "peft_model = get_peft_model(model, peft_cfg)\n",
        "\n",
        "# Print trainable parameters\n",
        "\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "131a51bc-89ad-4768-9487-0eed001508e8",
      "metadata": {
        "id": "131a51bc-89ad-4768-9487-0eed001508e8"
      },
      "outputs": [],
      "source": [
        "# --------- SFT training args  ---------\n",
        "args = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "\n",
        "    # dataset-sized schedule\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "\n",
        "    # stability & speed\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    bf16=use_bf16,\n",
        "    fp16=not use_bf16,\n",
        "    tf32=True,\n",
        "\n",
        "    # optimization\n",
        "    max_grad_norm=0.5,\n",
        "    learning_rate=2e-5,               # LoRA-friendly; fits higher capacity\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    optim=\"adamw_torch_fused\",          # fallback: \"adamw_torch\" if not supported\n",
        "    adam_beta1=0.9, adam_beta2=0.999,\n",
        "    adam_epsilon=1e-8,\n",
        "    weight_decay=0.1,\n",
        "\n",
        "    # logging/eval/save: keep it simple on small data\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    save_total_limit=2,\n",
        "\n",
        "    remove_unused_columns=False,\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "    dataset_text_field=\"\",  #   # <- stops TRL from looking for \"text\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bee45fa5-bc2c-46b6-a00e-f95213944113",
      "metadata": {
        "id": "bee45fa5-bc2c-46b6-a00e-f95213944113"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "803ea79a-7a0d-493c-82dc-4cb0f601cc01",
      "metadata": {
        "id": "803ea79a-7a0d-493c-82dc-4cb0f601cc01"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    args=args,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=evald,\n",
        "    data_collator=collate_fn,\n",
        "    #peft_config=peft_cfg,\n",
        ")\n",
        "trainer.add_callback(EarlyStoppingCallback(\n",
        "    early_stopping_patience=2,              # stop if no val-loss improvement for 2 evals\n",
        "    early_stopping_threshold=0.0\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95853f14-f3fa-4db9-a3ee-24962744e914",
      "metadata": {
        "id": "95853f14-f3fa-4db9-a3ee-24962744e914",
        "outputId": "f6614f46-b997-4239-88e5-dbf4d5f54199"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='906' max='906' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [906/906 4:12:52, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>11.561400</td>\n",
              "      <td>1.348950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>7.185900</td>\n",
              "      <td>0.862599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>6.336600</td>\n",
              "      <td>0.793658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>6.444200</td>\n",
              "      <td>0.775869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>5.903500</td>\n",
              "      <td>0.768302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>6.752400</td>\n",
              "      <td>0.763234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>6.101800</td>\n",
              "      <td>0.760473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>6.296600</td>\n",
              "      <td>0.759510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>6.784700</td>\n",
              "      <td>0.759124</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=906, training_loss=8.325606478760575, metrics={'train_runtime': 15187.6903, 'train_samples_per_second': 0.476, 'train_steps_per_second': 0.06, 'total_flos': 3.7215529310029824e+17, 'train_loss': 8.325606478760575})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb92b8c8-c0f6-47a5-830f-b9baa782d51d",
      "metadata": {
        "id": "fb92b8c8-c0f6-47a5-830f-b9baa782d51d",
        "outputId": "537e7f2d-d3cd-4b0b-8bc8-a50d6a4aafa0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.save_model(OUTPUT_DIR)      # saves the adapters\n",
        "processor.save_pretrained(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0fb9029-23fe-4111-bbbe-d99e4535ca98",
      "metadata": {
        "id": "d0fb9029-23fe-4111-bbbe-d99e4535ca98"
      },
      "source": [
        "## Inferance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0faeef34-d044-4985-9bae-f50220fa5441",
      "metadata": {
        "id": "0faeef34-d044-4985-9bae-f50220fa5441"
      },
      "outputs": [],
      "source": [
        "processor = AutoProcessor.from_pretrained(MODEL_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c28e17d-ea10-4251-af90-549c98e1c650",
      "metadata": {
        "id": "6c28e17d-ea10-4251-af90-549c98e1c650"
      },
      "outputs": [],
      "source": [
        "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "adapter_path = OUTPUT_DIR\n",
        "model.load_adapter(adapter_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b5ef6de-7470-4d0b-945e-17ddf08be798",
      "metadata": {
        "id": "2b5ef6de-7470-4d0b-945e-17ddf08be798"
      },
      "outputs": [],
      "source": [
        "def generate_text_from_sample(model, processor, sample, max_new_tokens=5000, device=\"cuda\"):\n",
        "    text_input = processor.apply_chat_template(build_messages(q=sample[\"question\"], a=\"\")[0], tokenize=False, add_generation_prompt=True)\n",
        "    # Process the visual input from the sample\n",
        "    image= clamp_long_edge(sample[\"image\"])\n",
        "\n",
        "    # Prepare the inputs for the model\n",
        "    model_inputs = processor(text=text_input, images=image, return_tensors=\"pt\").to(device)  # Move inputs to the specified device\n",
        "\n",
        "    gen_kwargs = {\"max_new_tokens\":max_new_tokens, \"do_sample\":False, \"pad_token_id\": processor.tokenizer.pad_token_id,\n",
        "                  \"eos_token_id\": processor.tokenizer.eos_token_id}\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        output_ids = model.generate(**model_inputs, **gen_kwargs)\n",
        "\n",
        "    # Decode only the generated part (exclude input tokens)\n",
        "    input_token_len = model_inputs[\"input_ids\"].shape[1]\n",
        "    generated_ids = output_ids[:, input_token_len:]\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = processor.decode(\n",
        "        generated_ids[0],\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=True\n",
        "    )\n",
        "\n",
        "    return generated_text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4835e3b3-cf1c-4f93-9297-88287bc1aa82",
      "metadata": {
        "id": "4835e3b3-cf1c-4f93-9297-88287bc1aa82",
        "outputId": "b8ff13d8-c162-4a73-a33b-6e6d4abe21dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The city with the highest parks density is Sydney, with 6.3 parks per square mile.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = generate_text_from_sample(model, processor, train[0])\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4720a35d-ed6a-4cdf-a6fa-5cd2aa6a0fae",
      "metadata": {
        "id": "4720a35d-ed6a-4cdf-a6fa-5cd2aa6a0fae"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
