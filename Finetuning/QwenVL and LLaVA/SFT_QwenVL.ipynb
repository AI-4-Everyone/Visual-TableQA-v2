{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1648798-37fa-400c-86d1-4e1891742532",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab9b94b-df1e-4a2d-be31-d3ade2b15fd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.6.0 torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3281e68-6ea0-4936-a044-59f743a28012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0+cu124torch2.6-cp310-cp310-linux_x86_64.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066dd584-f921-452d-9864-ae1bfc021bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U trl>=0.9.6 transformers>=4.42 peft>=0.12.0 accelerate>=0.33.0 bitsandbytes>=0.43.3 datasets>=2.18 qwen-vl-utils pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e630b-81f7-4bb5-97e7-c4fce3e39baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4abeec4d-af77-4b3a-acb4-bc4bc86b105c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "from transformers import (AutoProcessor, AutoTokenizer, BitsAndBytesConfig, Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor, EarlyStoppingCallback)\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from PIL import Image, ImageOps\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f12aea0-eba4-49eb-b563-896600eda737",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768eec0b-a4f3-49c5-908d-f5df4a74080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Config ---------\n",
    "MODEL_ID    = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "DATASET_REPO = \"AI-4-Everyone/Visual-TableQA\"\n",
    "#DATASET_REPO = \"hewei2001/ReachQA\"\n",
    "OUTPUT_DIR_TIER_A   = \"qwen-vl-sft-lora-tableqa-tierA\" #for Visual-TableQA\n",
    "#OUTPUT_DIR_TIER_A   = \"qwen-vl-sft-lora-reachqa-tierA\" #for ReachQA\n",
    "\n",
    "# --------- Load data ---------\n",
    "ds = load_dataset(DATASET_REPO)\n",
    "\n",
    "\"\"\"\n",
    "from datasets import ClassLabel\n",
    "labels = sorted(set(ds[\"train\"][\"qa_type\"]))\n",
    "ds = ds.cast_column(\"qa_type\", ClassLabel(names=labels))\n",
    "split = ds[\"train\"].train_test_split(test_size=0.1, seed=42, stratify_by_column=\"qa_type\")\n",
    "ds[\"train\"], ds[\"validation\"] = split[\"train\"], split[\"test\"]\n",
    "\"\"\"\n",
    "\n",
    "train = ds.get(\"train\")\n",
    "\n",
    "evald = ds.get(\"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d362b6-ea5d-41fc-93b2-6dd3ffd1c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from charts and diagrams images.\n",
    "Answer the questions strictly from the image, with clear, rigorous step-by-step justification. Stay concise, but include all reasoning thatâ€™s relevant.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81b9c7f5-a84d-4137-a40b-8bcf6985ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pil(img):\n",
    "    return img if isinstance(img, Image.Image) else Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a405846-5d38-4a6c-907f-c5b110209bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(sample):\n",
    "    return [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "        },\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": [\n",
    "                {\"type\": \"image\",\n",
    "                 \"image\": to_pil(sample[\"image\"]),},\n",
    "                {\"type\": \"text\",\n",
    "                 \"text\": sample[\"question\"],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\"role\": \"assistant\",\n",
    "         \"content\": [{\"type\": \"text\", \"text\": sample[\"answer\"]}],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e9e28bd-f877-4209-b798-b883c88d3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    \n",
    "    msgs = [format_data(sample) for sample in examples]\n",
    "    \n",
    "    texts = [processor.apply_chat_template(m, tokenize=False, add_generation_prompt=True) for m in msgs]\n",
    "        \n",
    "    image_inputs, _ = process_vision_info(msgs)\n",
    "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True, truncation=False, max_length=None,)\n",
    "\n",
    "    labels = batch[\"input_ids\"].detach().clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # collect image/video token ids in a model-agnostic way\n",
    "    img_ids = set()\n",
    "    for attr in (\"image_token_id\", \"video_token_id\"):\n",
    "        v = getattr(model.config, attr, None)\n",
    "        if v is not None:\n",
    "            img_ids.add(v)\n",
    "    # fallback to common special tokens if present\n",
    "    for tok in (\"<image>\", \"<img>\", \"<video>\", '<|vision_start|>', '<|vision_end|>'):\n",
    "        tid = processor.tokenizer.convert_tokens_to_ids(tok)\n",
    "        if tid not in (None, -1, processor.tokenizer.unk_token_id):\n",
    "            img_ids.add(tid)\n",
    "\n",
    "    if img_ids:\n",
    "        mask = torch.isin(labels, torch.tensor(sorted(img_ids), dtype=labels.dtype, device=labels.device))\n",
    "        labels[mask] = -100\n",
    "        \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f032867e-f15d-40ff-8bad-10d3a22cf3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_bf16 = torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e407dc-3dcb-4631-9fd8-0802051b9da7",
   "metadata": {},
   "source": [
    "## Tier A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ca503-1cb6-432a-ae7a-bc44d23c6872",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    "    # Optional (if your setup supports it):\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model.config.use_cache = False                      # crucial for training memory\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.pretraining_tp = 1  # ensure tensor parallelism is disabled\n",
    "\n",
    "min_pixels = 256*28*28\n",
    "max_pixels = 2560*28*28\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, min_pixels=min_pixels, max_pixels=max_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "771af569-2a12-4e73-8f7c-f6dc90b1110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- LoRA config (r=16, alpha=8) ---------\n",
    "TARGETS = [\n",
    "  \"q_proj\", \"v_proj\", \"k_proj\",\"o_proj\",\n",
    "  \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "  # optional but sometimes helpful\n",
    "  #\"lm_head\",\n",
    "  \"multi_modal_projector\",   # Qwen2 / Qwen2.5 VL (HF naming)\n",
    "  ]\n",
    "\n",
    "r, lora_alpha = 16, 8\n",
    "\n",
    "peft_cfg = LoraConfig(r=r, lora_alpha=lora_alpha, lora_dropout=0.05, bias=\"none\",\n",
    "                      target_modules=TARGETS, task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d396b84-5278-4fbc-a229-64afd27e42d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 47,589,376 || all params: 8,339,756,032 || trainable%: 0.5706\n"
     ]
    }
   ],
   "source": [
    "# Apply PEFT model adaptation\n",
    "\n",
    "peft_model = get_peft_model(model, peft_cfg)\n",
    "\n",
    "# Print trainable parameters\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "131a51bc-89ad-4768-9487-0eed001508e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR_TIER_A,\n",
    "\n",
    "    num_train_epochs=2,       #1 for ReachQA due to datasets sizes difference \n",
    "\n",
    "    # ---- batch / memory ----\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,   # global batch ~= 24\n",
    "    dataloader_pin_memory=True,\n",
    "\n",
    "    # ---- stability & speed ----\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    bf16=use_bf16,\n",
    "    fp16=not use_bf16,\n",
    "    tf32=True,\n",
    "\n",
    "    # ---- optimization (LoRA-friendly) ----\n",
    "    learning_rate=1e-4,             \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.07,              # 5â€“10% warmup works well\n",
    "    max_grad_norm=1.0,              \n",
    "    weight_decay=0.01,              \n",
    "    optim=\"adamw_torch_fused\",\n",
    "    adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-8,\n",
    "\n",
    "    # ---- logging / eval / save ----\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,                  \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    dataset_text_field=\"\",  # keep TRL from looking for \"text\"\n",
    ")\n",
    "\n",
    "# Add early stopping (patience 2 evals)\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.0)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee45fa5-bc2c-46b6-a00e-f95213944113",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "803ea79a-7a0d-493c-82dc-4cb0f601cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=evald,\n",
    "    data_collator=collate_fn,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95853f14-f3fa-4db9-a3ee-24962744e914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='205' max='275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [205/275 5:01:40 < 1:44:01, 0.01 it/s, Epoch 0.74/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.601800</td>\n",
       "      <td>0.627113</td>\n",
       "      <td>3.011601</td>\n",
       "      <td>5979027.000000</td>\n",
       "      <td>0.827814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.586300</td>\n",
       "      <td>0.606202</td>\n",
       "      <td>3.059967</td>\n",
       "      <td>11945729.000000</td>\n",
       "      <td>0.830864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92b8c8-c0f6-47a5-830f-b9baa782d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(OUTPUT_DIR_TIER_A)      # saves the adapters\n",
    "processor.save_pretrained(OUTPUT_DIR_TIER_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed61ac9c-bd7d-4fc1-a57b-2d51ce2b93c8",
   "metadata": {},
   "source": [
    "#### Resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ca46b78-9d8c-45f2-9fb0-3caab1dc7475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='549' max='550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [549/550 3:50:45 < 01:34, 0.01 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.538900</td>\n",
       "      <td>0.577489</td>\n",
       "      <td>3.019494</td>\n",
       "      <td>5962153.000000</td>\n",
       "      <td>0.834886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=549, training_loss=0.14997396399631743, metrics={'train_runtime': 13927.6756, 'train_samples_per_second': 0.946, 'train_steps_per_second': 0.039, 'total_flos': 1.706012246911703e+18, 'train_loss': 0.14997396399631743, 'epoch': 1.9981785063752278})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf620d9e-53ce-45b2-8f08-04ccd87eef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(OUTPUT_DIR_TIER_A)      # saves the adapters\n",
    "processor.save_pretrained(OUTPUT_DIR_TIER_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9918b3-6405-4ba3-8278-916894bf9432",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## B Tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f2e8420-ece3-49e0-97f3-9aca76f6d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR_TIER_B= \"qwen-vl-sft-lora-tableqa-tierB\" \n",
    "#OUTPUT_DIR_TIER_B= \"qwen-vl-sft-lora-reachqa-tierB\" #for ReachQA\n",
    "MERGED_DIR = \"qwen-vl-merged-tierA-tableqa\"             # <- new folder to save merged base\n",
    "#MERGED_DIR = \"qwen-vl-merged-tierA-reachqa\"             # #for ReachQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ae2ec-4141-490e-ac24-65a84fd7b260",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Merge Tier A adapter and base model (run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "555c3df3-c615-4839-8cb5-98a78d1abc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 5 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:50<00:00, 10.10s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    "    # Optional (if your setup supports it):\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model.config.use_cache = False                      \n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.pretraining_tp = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98aeb86d-f9d8-4246-bb23-544d8b3ed829",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = PeftModel.from_pretrained(model, OUTPUT_DIR_TIER_A, is_trainable=False)\n",
    "merged = peft_model.merge_and_unload() \n",
    "merged.save_pretrained(MERGED_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c932ffb4-b286-444b-a346-a91d375efdb5",
   "metadata": {},
   "source": [
    "### (re)load merged base for tier B phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5052c938-b181-461a-b76e-92543c32137b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:15<00:00,  3.99s/it]\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MERGED_DIR, device_map=\"auto\", torch_dtype=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\", low_cpu_mem_usage=True\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "min_pixels = 256*28*28\n",
    "max_pixels = 2560*28*28\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, min_pixels=min_pixels, max_pixels=max_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a787095-fbf3-40f0-bc92-bbdf7f9715c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tierB_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=32, lora_dropout=0.10, bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"attn.qkv\", \"attn.proj\"],\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, tierB_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c70ba55e-eb36-4fbc-87df-ec8a237ee856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 245,760 || all params: 8,294,132,736 || trainable%: 0.0030\n"
     ]
    }
   ],
   "source": [
    "for _, p in peft_model.named_parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "# capture indices from your paths: model.visual.blocks.<idx>.\n",
    "layer_re = re.compile(r\"model\\.visual\\.blocks\\.(\\d+)\\.\")\n",
    "\n",
    "# discover how many blocks the vision tower has\n",
    "vision_layers = sorted({int(m.group(1)) for n, _ in peft_model.named_modules()\n",
    "                        if (m := layer_re.search(n))})\n",
    "N = 4                                 # tune last 4; adjust 2â€“6 if needed\n",
    "lastN = set(vision_layers[-N:])\n",
    "\n",
    "def is_lastN_vision_attn_lora(name: str) -> bool:\n",
    "    if \"lora_\" not in name:\n",
    "        return False\n",
    "    m = layer_re.search(name)\n",
    "    if not m:\n",
    "        return False\n",
    "    idx = int(m.group(1))\n",
    "    if idx not in lastN:\n",
    "        return False\n",
    "    # only attention projections\n",
    "    return (\"attn.qkv\" in name) or (\"attn.proj\" in name)\n",
    "\n",
    "for n, p in peft_model.named_parameters():\n",
    "    if is_lastN_vision_attn_lora(n):\n",
    "        p.requires_grad_(True)\n",
    "\n",
    "#(Optional) keep the multimodal projector from Tier A trainable a bit more\n",
    "for n, p in peft_model.named_parameters():\n",
    "    if \"multi_modal_projector\" in n and (\"lora_\" in n) and (\"default\" in n):\n",
    "        p.requires_grad_(True)\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d2eda30-ed45-4bba-8c03-9d205d453c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "args_tierB = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR_TIER_B,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    bf16=use_bf16, fp16=not use_bf16, tf32=True,\n",
    "\n",
    "    learning_rate=2e-5,            # â†“ a bit for vision LoRA; safe starting point\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.07,\n",
    "    max_grad_norm=1.0,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    dataset_text_field=\"\",\n",
    ")\n",
    "\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae325b6f-4f61-45cd-8252-e5552dcc6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=args_tierB,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=evald,\n",
    "    data_collator=collate_fn,\n",
    "    processing_class=processor,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1960e06-55a0-4d86-a62f-950853f1addb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='275' max='275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [275/275 4:33:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.572100</td>\n",
       "      <td>0.607478</td>\n",
       "      <td>2.971453</td>\n",
       "      <td>5979027.000000</td>\n",
       "      <td>0.831044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.584300</td>\n",
       "      <td>0.607474</td>\n",
       "      <td>2.972311</td>\n",
       "      <td>11945729.000000</td>\n",
       "      <td>0.831172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR_TIER_B)      # saves the adapters\n",
    "processor.save_pretrained(OUTPUT_DIR_TIER_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb9029-23fe-4111-bbbe-d99e4535ca98",
   "metadata": {},
   "source": [
    "## Final Model Loading and Inferance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c28e17d-ea10-4251-af90-549c98e1c650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 5 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:59<00:00, 11.94s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "base = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MERGED_DIR, device_map=\"auto\", torch_dtype=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\", low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base, OUTPUT_DIR_TIER_B)\n",
    "\n",
    "del base                       \n",
    "torch.cuda.empty_cache()    \n",
    "model.eval()\n",
    "min_pixels = 256*28*28\n",
    "max_pixels = 2560*28*28\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, min_pixels=min_pixels, max_pixels=max_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b5ef6de-7470-4d0b-945e-17ddf08be798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_sample(model, sample, max_new_tokens=5000, device=\"cuda\"):\n",
    "    # Prepare the text input by applying the chat template\n",
    "    text_input = processor.apply_chat_template(format_data(sample)[:2], tokenize=False, add_generation_prompt=True)\n",
    "    # Process the visual input from the sample\n",
    "    image_inputs, _ = process_vision_info(format_data(sample)[:2])\n",
    "    \n",
    "    # Prepare the inputs for the model\n",
    "    model_inputs = processor(text=[text_input], images=image_inputs, return_tensors=\"pt\").to(device)  # Move inputs to the specified device\n",
    "\n",
    "    # Generate text with the model\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "\n",
    "    # Trim the generated ids to remove the input ids\n",
    "    trimmed_generated_ids = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "\n",
    "    # Decode the output text\n",
    "    output_text = processor.batch_decode(\n",
    "        trimmed_generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]  # Return the first decoded output text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4835e3b3-cf1c-4f93-9297-88287bc1aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate_text_from_sample(model, train[0])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858e290-5134-46c9-bd13-c9e2d9c609f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
